[![CI - Sentiment Analysis API](https://github.com/DataScientest-Studio/sep25_bde_satisfaction_b/actions/workflows/ci.yml/badge.svg)](https://github.com/DataScientest-Studio/sep25_bde_satisfaction_b/actions/workflows/ci.yml)
# Projet : Satisfaction Client dans la Supply Chain

## Pr√©sentation et Objectifs

Ce projet, r√©alis√© dans le cadre de la formation [**Data Engineer - DataScientest**](https://datascientest.com/), a pour objectif d‚Äôanalyser la **satisfaction client** √† partir d‚Äôavis collect√©s en ligne, notamment sur **Trustpilot** et d‚Äôautres plateformes de notation.

La **supply chain** englobe l‚Äôensemble du processus d‚Äôapprovisionnement, de production et de distribution d‚Äôun produit.\
L‚Äôanalyse de la satisfaction client permet d‚Äô√©valuer la qualit√© de cette cha√Æne en identifiant des probl√©matiques li√©es √† :

- la conception des produits,
- la logistique et les d√©lais de livraison,
- la tarification,
- la durabilit√©,
- ou encore la conformit√© du service aux attentes du march√©.

L‚Äôobjectif principal est donc de **mesurer, synth√©tiser et visualiser la satisfaction client**, tout en automatisant la collecte, le traitement et la mise √† jour des donn√©es.

---

## √âtapes du Projet

### 1. Extraction des Donn√©es

- **Objectif :** extraire des informations √† partir de sites comme Trustpilot.
- **M√©thodes :** web scraping et enregistrement des donn√©es dans des fichier CSV et JSON.
- **Livrables :**
  - Fichiers CSV et JSON,
  - Fichier explicatif du traitement (documentation technique).
- **Outils :** Python (requests, BeautifulSoup, Pandas) et no code (extension web scraper).

### 2. Organisation de la Donn√©e

- **Objectif :** concevoir une base de donn√©es relationnelle pour les informations sur les entreprises et une base orient√©e document pour les commentaires clients.
- **Livrables :**
  - Scripts SQL pour la cr√©ation et les requ√™tes,
  - Impl√©mentation ElasticSearch + dashboard Kibana.
- **Outils :** SQL, ElasticSearch, MongoDB (*√† confirmer*), Kibana.

### 3. Analyse et Machine Learning

- **Objectif :** effectuer une **analyse de sentiment** sur les avis collect√©s.
- **Livrables :** notebook comment√© avec les mod√®les d‚Äôanalyse.
- **Outils :** Python (Pandas, Scikit-learn, NLTK, TextBlob).

### 4. Mise en Production

- **Objectif :** exposer les mod√®les via une API et rendre le projet d√©ployable.
- **Livrables :**
  - API Flask ou FastAPI,
  - Conteneurisation Docker (Dockerfile + docker-compose).
- **Outils :** FastAPI, Docker, GitLab CI.

### 5. Automatisation et Monitoring

- **Objectif :** automatiser le scraping, le d√©ploiement et la surveillance du syst√®me.
- **Livrables :**
  - Pipeline CI/CD,
  - DAG Airflow ou CronJob,
  - Monitoring avec Prometheus / Grafana.
- **Outils :** Airflow, GitLab, Prometheus, Grafana.

---

## Installation et Lancement via Docker

L‚Äôapplication est enti√®rement conteneuris√©e pour simplifier le d√©ploiement et l‚Äôex√©cution sur tous les syst√®mes (Windows, Linux et macOS).

### Pr√©requis

Assurez-vous d‚Äôavoir install√© :

- [**Docker Desktop**](https://www.docker.com/products/docker-desktop/) (Windows / macOS)
- [**Docker Engine**](https://docs.docker.com/engine/install/) (Linux)

Pour v√©rifier l‚Äôinstallation :

```bash
docker --version
docker compose version
```

### Lancer le projet

1. **Cloner le d√©p√¥t :**

   ```bash
   git clone <lien_du_d√©p√¥t_github>
   cd sep25_bde_satisfaction_b
   ```

2. **D√©marrer le service ETL :**

   ```bash
   docker compose up -d
   ```

   Gr√¢ce au montage de volume de l‚Äôapplication, toute modification du code source est automatiquement prise en compte sans n√©cessiter de reconstruction compl√®te de l‚Äôimage.

   Une fois le conteneur lanc√©, les **biblioth√®ques Python n√©cessaires** sont automatiquement install√©es gr√¢ce au fichier `requirements.txt`, comme d√©fini dans le `Dockerfile`. Cela garantit que l‚Äôenvironnement √† l‚Äôint√©rieur du conteneur contient toutes les d√©pendances requises.

   **Extrait du `Dockerfile` :**
   ```dockerfile
   # Copier les d√©pendances
   COPY requirements.txt .
   RUN pip install --no-cache-dir -r requirements.txt
   ```

   Cet extrait montre que les d√©pendances list√©es dans `requirements.txt` sont copi√©es dans l‚Äôimage Docker, puis install√©es automatiquement lors du build initial. Ainsi, apr√®s le lancement de `docker compose up -d`, le conteneur pr√©pare l‚Äôenvironnement Python avant d‚Äôex√©cuter le script principal.

   **Extrait du `docker-compose.yml` :**
   ```yaml
   build: .
   volumes:
     - .:/app
   ```

   L‚Äôinstruction `build: .` indique √† Docker d‚Äôutiliser le `Dockerfile` situ√© √† la racine du projet pour construire l‚Äôimage, tandis que le volume `.:/app` permet de synchroniser les fichiers locaux avec ceux du conteneur.

3. **Exc√©cuter `etl.py` dans le CLI du conteneur :**
   
   Pour interagir directement avec le conteneur et ex√©cuter des commandes √† l‚Äôint√©rieur, il est possible d‚Äôouvrir un shell (CLI) via la commande suivante :
   ```bash
   docker exec -it satisfaction_client_etl bash
   ```

   Une fois dans le CLI du conteneur, ex√©cute le script principal `etl.py` avec :
   ```bash
   python etl.py
   ```
   
   Cette commande lance le script ETL et permet de v√©rifier son bon fonctionnement.
   Tu devrais voir appara√Ætre dans la console le message d√©fini dans ton code, par exemple :
      ```bash
   D√©marrage du script ETL...
   ```

4. **Arr√™ter les conteneurs :**
   ```bash
   docker compose down
   ```

> ‚ÑπÔ∏è Le `docker-compose.yml` est configur√© pour un environnement de d√©veloppement. Il sera enrichi progressivement pour inclure d‚Äôautres services (API, base de donn√©es, monitoring, etc.).

---

## Ajouter du code et g√©rer les imports dans `etl.py`

Le fichier principal `etl.py` est le point d‚Äôentr√©e du pipeline ETL. C‚Äôest ici que sont orchestr√©es les diff√©rentes √©tapes d‚Äôextraction, de transformation et de chargement des donn√©es.

### Structure typique du projet

```
project_root/
‚îÇ
‚îú‚îÄ‚îÄ extract/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ scrape_compagnies.py
‚îÇ   ‚îú‚îÄ‚îÄ scrape_reviews.py
‚îÇ
‚îú‚îÄ‚îÄ notebooks/
‚îÇ
‚îú‚îÄ‚îÄ etl.py
‚îú‚îÄ‚îÄ Dockerfile
‚îú‚îÄ‚îÄ docker-compose.yml
‚îú‚îÄ‚îÄ requirements.txt
‚îî‚îÄ‚îÄ .env
```

### Ajouter du code dans `etl.py`

Pour int√©grer une nouvelle fonctionnalit√© (par exemple une fonction de nettoyage ou une nouvelle source de donn√©es), cr√©ez un fichier Python dans le dossier `extract/` et importez-le ensuite dans `etl.py`.

**Exemple :**

Dans `extract/scrape_reviews.py` :

```python
def scrape_reviews():
    print("Scraping des avis clients...")
```

Dans `etl.py` :

```python
from extract.scrape_reviews import scrape_reviews

if __name__ == "__main__":
    scrape_reviews()
```

Gr√¢ce au montage de volume d√©fini dans `docker-compose.yml`, toute modification locale dans ces fichiers est imm√©diatement prise en compte par le conteneur. Il n‚Äôest donc **pas n√©cessaire de reconstruire l‚Äôimage** pour tester de nouvelles fonctions.

> üí° **Astuce :** Assurez-vous que chaque module Python contient un fichier `__init__.py` (m√™me vide) pour que Python reconnaisse le dossier comme un package importable.

---

## Gestion des variables d‚Äôenvironnement (.env)

Le projet inclut un fichier `.env` pour centraliser les variables sensibles (mots de passe, identifiants API, configurations de base de donn√©es, etc.).

### Pourquoi utiliser un fichier `.env` ?

Avoir un fichier `.env` permet d‚Äôadopter une approche **professionnelle et s√©curis√©e** :
- Les mots de passe et cl√©s API **ne doivent jamais √™tre partag√©s** ni commit√©s sur GitHub.
- Les informations sensibles peuvent √™tre facilement modifi√©es sans impacter le code source.
- Cela facilite la gestion des environnements (d√©veloppement, test, production).

### Exemple d‚Äôutilisation

**Fichier `.env` :**
```bash
DB_USER=my_user
DB_PASSWORD=my_password
API_KEY=abc123xyz
```

**Utilisation dans le code Python :**
```python
from dotenv import load_dotenv
import os

load_dotenv()

user = os.getenv("DB_USER")
password = os.getenv("DB_PASSWORD")
api_key = os.getenv("API_KEY")
```

**R√©f√©rence dans le `docker-compose.yml` :**
```yaml
env_file:
  - .env
```

> ‚ö†Ô∏è Le fichier `.env` doit √™tre ajout√© dans le `.gitignore` pour √©viter toute fuite de donn√©es sensibles.

---

## Bonnes pratiques Docker

- Utiliser des **volumes mont√©s** pour permettre le rechargement automatique du code sans rebuild.
- √âviter les reconstructions inutiles (`docker compose up` suffit pour appliquer les changements).
- Utiliser des **variables d‚Äôenvironnement** pour distinguer les contextes (d√©veloppement, test, production).
- Ex√©cuter les processus sous un **utilisateur non-root** pour des raisons de s√©curit√©.
- Structurer le projet avec des dossiers d√©di√©s (`/extract`, `/data`, `/api`, `/notebooks`, etc.) pour faciliter la maintenance.

---

## √âquipe du Projet

- Ousmane Ibrahima SY [LinkedIn](https://www.linkedin.com/in/ousmane-sy-6926a6139) / [GitHub](https://github.com/Oussouke)
- Arnaud GUILLOUX [LinkedIn](https://www.linkedin.com/) / [GitHub](https://github.com/)
- Rodolphe Katz [LinkedIn](https://www.linkedin.com/) / [GitHub](https://github.com/)

